---
title: "Homework on optimization algorithms."
date: "P8160 Advanced Statistical Computing "
output: pdf_document #html_document
---

```{r setup, include=FALSE}
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(reticulate)
library(tidyverse)
knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = T,
  warning = F,
  cache = F
)
theme_set(theme_minimal() + theme(
  legend.position = "bottom",
  plot.title = element_text(hjust = 0.5)
))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)


scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(2021)
```


## Problem 1: 
Design an optmization algorithm to find the minimum of the continuously differentiable function 
$$f(x) =-e^{-x}\sin(x)$$ on the closed interval $[0,1.5]$. Write out your algorithm and implement it into \textbf{R}.


# Answer: your answer starts here...

To find the minimum of a continuously function,we first make some changes to the function let

$$g(x) = e^{x}\sin(x)$$

and instead find the maximum of g(x).

The gradient of $g(x)$ is :
$$\nabla g(x) = e^x(\sin(x)+\cos(x))$$

```{r}
ggplot(tibble(x = seq(0,1.5,length = 10)),aes(x))+
  geom_function(fun = function(x) exp(x)*(sin(x)+cos(x)))
```

and the Hessian is:
$$\nabla^2g(x) = 2e^x\cos(x)$$
```{r}
plot(function(x) 2*exp(x)*cos(x),xlim = c(0,1.5))
```
the hessian is greater than 0 everywhere in [0,1.5], so we can't use Newton method.

```{r golden_search}
goose_egg = 
  function(
    fun,
    left = NULL,
    right = NULL,
    range = NULL,
    ratio = 0.618,
    tol = 10e-4,
    ...
  ){
    if (!any(left,right)){
      left = range[1]
      right = range[2]
    }
    
    mid_1 = left + ratio*(right - left)
    
    f_mid_1 = fun(mid_1)
    
    mid_2 = mid_1 + ratio*(right-mid_1)
    
    f_mid_2 = fun(mid_2)
    
    f_left = fun(left)
    
    f_right = fun(right)
    
    i = 1
    
    while (abs(f_left - f_right)>tol && i<1000){
      i = i + 1
      if (f_mid_1 < f_mid_2) {
        f_left = f_mid_1
        left = mid_1
      } else {
        f_right = f_mid_2
        right = mid_2
      }
      mid_1 = left + ratio * (right - left)
      f_mid_1 = fun(mid_1)
      mid_2 = mid_1 + ratio * (right - mid_1)
      f_mid_2 = fun(mid_2)
    }
    return(mean(mid_1,mid_2))
  }
```


```{r implement}
x_max = goose_egg(function(x) exp(x)*sin(x),range = c(0,1.5))

print(x_max)

plot(x_max,-exp(x_max)*sin(x_max))

plot(function(x) {-exp(x)*sin(x)}, xlim = c(0,1.5), add = T)

```


## Problem 2:

The Poisson distribution, written as  
$$P(Y=y) = \frac{\lambda^y e^{-\lambda}}{y!}$$
for $\lambda > 0$,
is often used to model ``count'' data ---
e.g., the number of events in a given time period. 

A Poisson regression model states that
$$Y_i \sim \textrm{Poisson}(\lambda_i),$$
where
$$\log \lambda_i = \alpha + \beta x_i $$
for some explanatory variable $x_i$.  The question is how to estimate $\alpha$ and $\beta$ given a
set of independent data $(x_1, Y_1), (x_2, Y_2), \ldots, (x_n, Y_n)$.


\begin{enumerate}
\item Generate a random sample $(x_i, Y_i)$ with $n=500$ from the Possion regression model above. 
You can choose the true parameters $(\alpha,\beta)$ and the distribution of $X$.

\item Write out the likelihood of your simulated data, and its Gradient and Hessian functions. 

\item  Develop a modify Newton-Raphson algorithm that allows the step-halving and re-direction steps
to ensure ascent directions and monotone-increasing properties. 

\item Write down your algorithm and implement it in R to estimate $\alpha$ and $\beta$ from your simulated data.
\end{enumerate}

# Answer: your answer starts here...

## 2.1
```{python eval =F}
print("hello world")
```

```{r}
X = rbind(rep(1,500),rnorm(500))
Beta = runif(2)
lambda = exp(t(X)%*%Beta)
Y = map(lambda,~rpois(1,.x)) %>% unlist()
dat = list(y = Y, x=X)
ans = glm(Y~0+t(X),family = poisson())
```

## 2.2

- The log-likelihood of Poisson distribution is 
$$l(Y;\lambda) = \sum \{y*log(\lambda) -\lambda - log(y!)\}$$ 
OR
$$l(Y;\alpha,\beta) = \sum \{y*(\alpha+x\beta) -exp(\alpha+x\beta) - log(y!)\}$$

- The Score funtion is 
$$\nabla(Y;\alpha,\beta) = \frac{\partial}{\partial\lambda}l(Y;\lambda) = (\sum\{y-exp(\alpha+x\beta)\},\sum\{y*x-x*exp(\alpha+x\beta)\})$$

- The hessian is 
$$\nabla^2(Y;\lambda) = \frac{\partial^2}{\partial\lambda^2}l(Y;\lambda)$$

$$
=
\begin{pmatrix}
\sum{-exp(\alpha+x\beta)} & \sum{-x*exp(\alpha+x\beta)} \\
\sum{-x*exp(\alpha+x\beta)} & \sum{-x^2*exp(\alpha+x\beta)}
\end{pmatrix}
$$
which is negative defined everywhere.

```{r poisson}
Poisson =
  function(Y,X,
           theta_vec) {
    lambda = exp(t(X) %*% theta_vec)
    loglink = sum(Y * log(lambda) - lambda - log(factorial(Y)))
    
    fisher = var(Y * log(lambda) - lambda - log(factorial(Y)))
    
    gradient = c(sum(Y - lambda), sum(Y * X[2,] - X[2,] * lambda))
    
    hessian = matrix(c(
      sum(-lambda),
      sum(-X[2,] * lambda),
      sum(-X[2,] * lambda),
      sum((-X[2,] ^ 2) * lambda)
    ), ncol = 2)
    
    return(list(
      loglink = loglink,
      fisher = fisher,
      gradient = gradient,
      hessian = hessian
    ))
  }

Poisson(Y,X,c(7,2))
```

## 2.3

the Newton method updating is:
$$\nabla g(x_{k+1}) = \nabla g(x_k) + \eta*\nabla^2 g(x_k)(x_{k+1}-x_k)$$
where $\eta$ is the step size that ensure $\nabla g(x_{k+1}) > \nabla g(x_k)$




```{r newton}
#Develop a modify Newton-Raphson algorithm that allows the 
#step-halving and 
#re-direction steps
#to ensure ascent directions and monotone-increasing properties. 

newton_update =
  function(fun,
           previous_theta,
           y,x,
           step_size = 1,
           optimizer = F,
           backtracking = T,
           tol = 1e-8) {
    #take previous gradient and a updated hessian, return update gradient with
    #backtracking
    #if (abs(fun(y,x,previous_theta)$loglink) == Inf) stop("Check your log-likelihood")
    trial = 0
    
    gradient = fun(y,x,previous_theta)$gradient
    
    if (is.function(optimizer)) {
      hessian = optimizer(y,x, fun,) # get H
    } else{
      if (is.numeric(optimizer)) {
        H = optimizer # use H
      } else{
        hessian = fun(y,x, previous_theta)$hessian
        H = solve(hessian)
        while (all(eigen(H)$values > 0)) {# eigen decomposition
          P = eigen(hessian)
          lambda = max(P$values)
          hessian =
            P$vectors %*% (diag(P$values) - (lambda + tol) * diag(length(P$values))) %*%
            solve(P$vectors)
          
          H = solve(hessian)
        }
      }
    }
    
    #updating
    cur_theta = previous_theta - step_size * H %*% gradient
    
    #backtracking
    while (backtracking & fun(y,x,cur_theta)$loglink < fun(y,x,previous_theta)$loglink & trial < 2000) {
      step_size = step_size / 2
      
      trial = trial + 1 # avoild dead loops
      
      cur_theta = previous_theta - step_size * H %*% gradient
    }
    
    return(cur_theta)
  }

newton_update(fun = Poisson,previous_theta = c(7,2), y=Y,x =X)
```

```{r echo = F, eval = F}
BFGS = function(theta_1,Y_1,theta_2,Y_2,theta_3,Y_3){
  H_old = 
}
```



```{r naive_newton}
naive_newton =
  function(fun,
           init_theta = 1,
           y,x,
           tol = 1e-8,
           maxtiter = 2000,
           optimizer = F,
           ...) {
    
    f = fun(y,x,init_theta)
    
    if (any(is.null(f$loglink),
            is.null(f$gradient),
            is.null(f$hessian))) {
      stop("fun input must return both gradient and hessian")
    }
    result = tibble()
    
    i = 0
    
    cur_theta = init_theta
    
    prevlog = -Inf # \nabla g(x_{k})
    
    while (any(abs(f$loglink)==Inf,abs(f$loglink - prevlog) > tol) && i < maxtiter) {
      i = i + 1
      prev_theta = cur_theta
      prevlog = f$loglink
      cur_theta = newton_update(fun, prev_theta,y,x)
      f = fun(y,x,cur_theta)
      result =
        rbind(result, tibble(
          iter = i,
          x_i = list(prev_theta),
          `g(x_i)` = prevlog
        ))
    }
    return(list(theta = cur_theta,result = result))
  }
```

```{r}
Beta_hat = naive_newton(Poisson,init_theta = c(7,2),Y,X)$theta

tibble(
  term = c("alpha", "beta"),
  theta = Beta,
  theta_hat = Beta_hat
) %>%
  knitr::kable()
```



## Problem 3: 
```{r echo = F}
breast = 
  read_csv("./breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  select(diagnosis:fractal_dimension_mean) %>% 
  mutate(diagnosis = case_when(
    diagnosis == "M"~1,
    diagnosis != "M" ~0)#forcats::fct_relevel(diagnosis,"M")
         )

Y = breast$diagnosis

X = model.matrix(diagnosis~.,breast)[,-1]

X_g = MASS::mvrnorm(100,rep(0,10),diag(10))
Beta = runif(10,0,10)
Y_g = X_g%*%Beta  + rnorm(100)
```


The data \textit{breast-cancer.csv} have 569 row and 33 columns. The first column \textbf{ID} lables individual breast tissue images; The second column \textbf{Diagnonsis} indentifies if the image is coming from cancer tissue or benign cases (M=malignant, B = benign). There are 357  benign and 212  malignant cases. The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei;
\begin{itemize}
\item radius (mean of distances from center to points on the perimeter)
\item texture (standard deviation of gray-scale values)
\item perimeter
\item area
\item smoothness (local variation in radius lengths)
\item compactness (perimeter\^ 2 / area - 1.0)
\item concavity (severity of concave portions of the contour)
\item concave points (number of concave portions of the contour)
\item symmetry
\item fractal dimension ("coastline approximation" - 1)
\end{itemize}


The goal  is to build a predictive model based on logistic regression to facilitate cancer diagnosis; 


\begin{enumerate}
\item Build a logistic model to classify the images into  malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.  

\item Build a logistic-LASSO model to select features, and implement a path-wise coordinate-wise optimization algorithm to obtain a path of solutions with a sequence of descending $\lambda$'s. 


\item Write a report to summarize your findings.
\end{enumerate}

# 3

## 3.1

the data is a binomial outcome response, which follows a Bernoulli distribution,
Using logit link, which 
$$log(\frac{p}{1-p}) = X^T\beta$$
s.t 
$$p = \frac{exp(X\beta)}{1+exp(X\beta)}$$
the log-likelihood of Bernoulli is 
$$l(Y;\beta) = \sum\{y*log(\frac{p}{1-p})+log(1-p)\}=\sum\{y*X^T\beta-log({1+exp(X^T\beta)})\}$$
The gradient is
$$\nabla l(Y;\beta) = (\frac{\partial}{\partial \beta_i}l(Y;\beta))=(\sum(y*x_i - \frac{x_i exp(X^T\beta)}{1+exp(X^T\beta)})$$
and the hessian is 
$$\nabla^2 l(Y;\beta)=(\sum  -\frac{x_i*x_j*exp(X^T\beta)}{(1+exp(X^T\beta)^2)})$$

```{r i_dont_know}
Bernoulli = 
  function(y,x,
           theta_vec){
    if (length(theta_vec)!=ncol(x)) stop("length of theta_vec must match dim of x")
    if (is.factor(y)) y = y %>% as.numeric()-1
    Y = y
    X = x
    loglink = sum(Y * X%*%theta_vec - log(1+exp(X%*%theta_vec)))
    
    if (abs(loglink) == Inf){
     stop("Choose a better starting value")
    }
    
    fisher = var(Y * X%*%theta_vec - log(1+exp(X%*%theta_vec)))
    
    X = x*1e-0
    
    gradient = map(1:length(theta_vec),
                   ~ sum(Y * X[, .x] - X[, .x] * exp(X %*% theta_vec) /
                           (1 + exp(X %*% theta_vec)))) %>% unlist()
    
    hessian =
      expand.grid(i = seq(1,length(theta_vec)),
                  j = seq(1,length(theta_vec))) %>% 
      summarise(beta = map2(i,j,~sum(-X[,.x]*X[,.y]*exp(X%*%theta_vec)/(1+exp(X%*%theta_vec))^2))) %>%
      unnest(beta) %>% 
      pull(beta)
    
    hessian = matrix(hessian,ncol = length(theta_vec))
                 
    return(
      list(loglink = loglink,
           fisher = fisher,
           gradient  = gradient*1e+0,
           hessian = hessian*1e+0)
    )
  }

 Bernoulli(Y,X,rep(0,10))
# Bernoulli(dat,ans$coefficients[-1] %>% as.vector())
```
## 3.2
```{r}
soft_threshold =
  function(beta, lambda) {
    if (abs(beta)>lambda){
      if (beta >0) return(beta - lambda)
      return(beta +lambda)
    }
    return(0)
    }
```


```{r lasso,echo=F,eval=F}
lasso_update_fit =
  function(fun, y,x, theta_vec = NaN,lambda = 1,maxiter = 200,tol = 1e-6,... ) {
    # data preprocessing
    if (is.factor(y)) y = y %>% as.numeric() -1
    y = y
    x = scale(x)
    xsd = attr(x,"scaled:scale") %>% as.vector()
    x = cbind(rep(1,length(y)),x) # add alpha
    xsd = append(1,xsd)
    # checking if intercept is include
    beta_0 = mean(y)/(1-mean(y)) %>% log()
    if (any(is.na(theta_vec))) theta_vec = rep(beta_0,ncol(x))
    if (length(theta_vec)<ncol(x)) theta_vec = append(beta_0,theta_vec)

    iter = 0
    
    cur_result = fun(y,x, theta_vec)$loglink
    
    prev_result = -Inf
    
    result = tibble()
    while (any(abs(cur_result) == Inf,abs(cur_result - prev_result) > tol) && iter < maxiter) {
      prev_result = cur_result
      iter = iter + 1
      
      for (i in 1:length(theta_vec)) {
        #coordinate update beta not intercept
        theta_old = theta_vec[i] 
        cur_f = fun(y,x, theta_vec)
        cur_result = cur_f$loglink
        gradient_i = cur_f$gradient[i]
        hessian_i = cur_f$hessian[i, i]
        H_i = solve(hessian_i)
        if (H_i>0) H_i = -1
        theta_new = theta_old - H_i %*% gradient_i
        if (i>1) theta_new = soft_threshold(theta_new, lambda) 
        # don't penalize intercept
        theta_vec[[i]] = theta_new
      }

     # result = rbind(result,
     #                tibble(
     #                  iteration = iter,
     #                  theta_id = list(1:length(theta_vec)),
     #                  theta = list(theta_vec*xsd),
     #                  L1 = fun(y,x, theta_vec)$loglink
     #                ))
    }
    return(list(theta = theta_vec/xsd, result = result))
  }

lasso_update = function(fun, y,x, theta_vec = NaN,
                        lambda = exp(seq(from = 5, to = -10)), ...) {
  lambda = lambda %>% as.vector()
  result = tibble()
  for (lambda_i in lambda) {
    result = rbind(result,
                   tibble(
                     lamdba = lambda_i,
                     result = lasso_update_fit(fun, y, x, theta_vec = theta_vec, lambda = lambda_i)
                   ))
  }
  return(result)
}


b = glmnet(X,Y,family="binomial",lambda = 0.1)

tibble(
  lasso_01lambda = lasso_update_fit(Bernoulli,Y,X,lambda = 0.5)$theta,
  glmnet = coef(b) %>% as.vector(),
  lasso_0lambda = lasso_update_fit(Bernoulli,Y,X,lambda = 0)$theta,
  newton = naive_newton(Bernoulli, init_theta = rep(0, 11), Y, cbind(rep(1, length(
    Y
  )), X))$theta,
  glm = glm(diagnosis ~ ., data = breast, family = binomial())$coefficient %>% as.vector()
) %>%
  knitr::kable()
```

```{r family_function}
bfun =
  function(y, x, theta_vec) {
    if (is.factor(y))
      y = y %>% as.numeric() - 1
    
    p_prev = exp(x %*% theta_vec) / (1 + exp(x %*% theta_vec)) #  row matrix
    
    p_prev[p_prev<1e-5] = 1e-5
    
    w_prev = p_prev * (1 - p_prev) # row matrix
    
    w_prev[w_prev<1e-5] = 1e-5
    
    z_prev = x %*% theta_vec + (y - p_prev) / w_prev # row * 1 matrix
    
    loglink = 
      sum(w_prev * (z_prev - x %*% theta_vec) ^ 2) / (2 * length(y))
    
    return(list(
      loglink = loglink,
      p = p_prev,
      w = w_prev,
      z = z_prev
    ))
  }

gfun = function(y,x,theta_vec){
  z = y
  w = rep(1/length(y),length(y))
  loglink = sum(w *(y - x %*% theta_vec) ^ 2) / 2
  return(
    list(z=z,w=w,loglink=loglink)
  )
}


```


```{r quadratic_lasso}
Qlasso =
  function(y,
           x,
           family_fun,
           lambda = 0,
           theta_vec = NA,
           maxiter = 500,
           tol = 1e-6,
           intercept = T,
           ...) {
    # data preparation
    if (is.factor(y))
      y = y %>% as.numeric() - 1
    y = y
    x = scale(x)
    xsd = attr(x, "scaled:scale") %>% as.vector()
    if (intercept) {
      x = cbind(rep(1, length(y)), x) # add alpha
      xsd = append(1, xsd)
    }
    
    # Manually choosing starting value
    if (!is.na(theta_vec)) {
      if (length(theta_vec) != ncol(x))
        theta_vec = append(rep(0, ncol(x) - length(theta_vec)), theta_vec)
    } else
      theta_vec = rep(0, ncol(x))
    
    #update part
    iter = 0

    cur_result = family_fun(y, x, theta_vec)$loglink
    
    if (abs(cur_result) == Inf|is.na(cur_result))
      stop("Diverge at starting value")
    
    prev_result = -Inf
    
    while (abs(cur_result - prev_result) > tol
           && iter < maxiter) {
      iter = iter + 1
      
      prev_result = cur_result
      
      for (i in 1:length(theta_vec)) {
        #weight
        fun_prev = family_fun(y, x, theta_vec)
        z_prev = fun_prev$z # working response
        w_prev = fun_prev$w # weight
        
        #update
        cur_theta =sum((z_prev - x[, -i] %*% theta_vec[-i]) * x[, i] * w_prev)
        
         #soft-threshold
        if (i > 1) {
          cur_theta =
            soft_threshold(cur_theta,
                           lambda) / sum(w_prev * (x[, i] ^ 2))
        } else {
          cur_theta = cur_theta / sum(w_prev * (x[, i] ^ 2))
        }
       
        #update theta
        theta_vec[[i]] = cur_theta
      }
      cur_result = family_fun(y, x, theta_vec)$loglink + lambda * sum(abs(theta_vec))
    }
    return(list(coefficient = theta_vec / xsd,
                loglikelihood = cur_result))
  }
```

```{r cv_function}
cv_Qlasso = 
  function(y,x,family_fun,lambda=NA,number = 5,intercept = T){
    result = tibble()
    lambda = as.vector(lambda)
    if (all(is.na(lambda))) {
      Y = y
      if (is.factor(y)) Y = y %>% as.numeric()-1
      Y = as.vector(Y)
      max_lambda = length(Y)*max(colSums(diag(Y) %*% scale(x)))
      lambda = exp(seq(log(max_lambda),-10,length=20))
    }
    block_length = length(y)/number
    for (lam in lambda) {
      for (i in 1:number) {
        #create training partition
        test_index = (1 + (number-1)*block_length):(number*block_length)
        trainX = x[-test_index,]
        testX = x[test_index,]
        trainY = y[-test_index]
        testY = y[test_index]
        
        #train
        trainF = Qlasso(trainY,trainX,family_fun,lambda = lam,intercept = intercept)
        if (intercept) {prev_coef = trainF$coefficient[-1]
        }else {prev_coef = trainF$coefficient}
        #test
        testF = family_fun(testY,testX,prev_coef)
        result = 
          result %>% 
          rbind(expand.grid(coefficient = trainF$coefficient,
                            lambda = lam,
                            loglikelihood = testF$logli+ lam*sum(abs(trainF$coefficient))) %>% 
                  cbind(tibble(term = (1:length(trainF$coefficient))-1) %>% 
                          mutate_all(as.character))
                )
      }
    }
    # take the mean of everything
    result = result %>% 
      group_by(lambda,term) %>% 
      summarise(across(c(coefficient,loglikelihood),mean))%>% 
      arrange((loglikelihood)) %>% 
      nest(coef = c(coefficient,term))
    
    coef = result[1,] %>% 
      unnest() %>% 
      pull(coefficient)
    
    lambda = result[1,"lambda"] %>% as.numeric()
    
    loglikelihood = result[1,"loglikelihood"] %>% as.numeric()
    
    return(list(
      coefficient = coef,
      loglikelihood = loglikelihood,
      lambda = lambda,
      cvtable = result
    ))
  }
```


```{r testing_1}
tibble(glm = glm(Y_g~X_g)$coefficient,
       glmnet_0lambda = coef(glmnet(X_g,Y_g,lambda = 0)) %>% as.vector(),
       Qlasso_0lambda = Qlasso(Y_g,X_g,gfun)$coefficient,
       Qlasso_01lambda = Qlasso(Y_g,X_g,gfun,5)$coefficient,
       glmnet_01lambda = coef(glmnet(X_g,Y_g,lambda = 5)) %>% as.vector()
       ) %>% 
  knitr::kable(caption = "Function comparison at Gaussian Family")

tibble(glm = glm(Y~X,family = binomial())$coefficient,
       glmnet_0lambda = coef(glmnet(X,Y,"binomial",lambda = 0)) %>% as.vector(),
       Qlasso_0lambda = Qlasso(Y,X,bfun)$coefficient,
       Qlasso_56.9lambda = Qlasso(Y,X,bfun,56.9)$coefficient,
       glmnet_01lambda = coef(glmnet(X,Y,"binomial",lambda =0.1)) %>% as.vector()
       ) %>% 
  knitr::kable(caption = "Function comparision at binomial family")
```

\ From above tables we can see that the `Qlasso` function of mine works reasonably close to `glmnet` except for the intercept terms in the case of Gaussian family. However, the result of binomial family behave rather different with L1 penalties. The function produce similar result with lambda = 0 except for `intercept` and once adding lamdba, the effect of lambda is rather different to `glmnet`.

```{r}
gaussian_Qlasso = cv_Qlasso(Y_g,X_g,gfun)

gaussian_Qlasso$cvtable %>% 
  unnest(coef) %>% 
  filter(term != 0) %>% 
  ggplot(aes(x = log(lambda), y = coefficient, color = term)) +
  geom_line() +
  scale_x_reverse() +
  labs(title = "Cross validate Qlasso selection at Gaussian Family")

gaussian_glmnet = cv.glmnet(X_g,Y_g)

plotmo::plot_glmnet(gaussian_glmnet$glmnet.fit)

tibble(term = c("intercept",colnames(X_g %>% as_tibble())),
       Qlasso = gaussian_Qlasso$coef,
       glmnet = coef(gaussian_glmnet) %>% as.vector()) %>% 
  knitr::kable(caption = "Gaussian family result")
```

 Using cross validation we can see that for gaussian, the penalty in the same scale as the `glmnet`, but because of the intercept is different, the optimum model chosen is different. same result(bug) can be observed in the binomial family, but rather deteriorate. The penalty although work similar to `glmnet` in terms of trends, but on a rather different scale(sample size). Also, the intercept produce by the function is not even the $log(\frac{\bar{Y}}{1-\bar{Y}})|X=0$, so the objective function failed to aid the function to choose the optimum model. 


```{r}
binomial_Qlasso = cv_Qlasso(Y,X,bfun)

binomial_Qlasso$cvtable %>% 
  unnest(coef) %>% 
  filter(term != 0) %>% 
  ggplot(aes(x = log(lambda/length(Y)), y = coefficient, color = term)) +
  geom_line() +
  scale_x_reverse() +
  labs(title = "Cross validate Qlasso selection at Binomial Family")

binomial_glmnet = cv.glmnet(X,Y,family = "binomial")

plotmo::plot_glmnet(binomial_glmnet$glmnet.fit)

tibble(term = c("intercept",colnames(X)),
       Qlasso = binomial_Qlasso$coef,
       glmnet = coef(binomial_glmnet) %>% as.vector()) %>% 
  knitr::kable(caption = "Binomial family result")
```

## 3.3

The result from glmnet shows that only radius, texture, smoothness and concave point and symetric
