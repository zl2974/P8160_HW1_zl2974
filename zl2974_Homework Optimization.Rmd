---
title: "Homework on optimization algorithms."
date: "P8160 Advanced Statistical Computing "
output: pdf_document #html_document
---

```{r setup, include=FALSE}
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(reticulate)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE,eval = T)

set.seed(2021)
```


## Problem 1: 
Design an optmization algorithm to find the minimum of the continuously differentiable function 
$$f(x) =-e^{-x}\sin(x)$$ on the closed interval $[0,1.5]$. Write out your algorithm and implement it into \textbf{R}.


# Answer: your answer starts here...

To find the minimum of a continuously function,we first make some changes to the function let

$$g(x) = e^{x}\sin(x)$$

and instead find the maximum of g(x).

The gradient of $g(x)$ is :
$$\nabla g(x) = e^x(\sin(x)+\cos(x))$$

```{r}
ggplot(tibble(x = seq(0,1.5,length = 10)),aes(x))+
  geom_function(fun = function(x) exp(x)*(sin(x)+cos(x)))
```

and the Hessian is:
$$\nabla^2g(x) = 2e^x\cos(x)$$
```{r}
plot(function(x) 2*exp(x)*cos(x),xlim = c(0,1.5))
```
the hessian is greater than 0 everywhere in [0,1.5], so we can't use Newton method.

```{r golden_search}
goose_egg = 
  function(
    fun,
    left = NULL,
    right = NULL,
    range = NULL,
    ratio = 0.618,
    tol = 10e-4,
    ...
  ){
    if (!any(left,right)){
      left = range[1]
      right = range[2]
    }
    
    mid_1 = left + ratio*(right - left)
    
    f_mid_1 = fun(mid_1)
    
    mid_2 = mid_1 + ratio*(right-mid_1)
    
    f_mid_2 = fun(mid_2)
    
    f_left = fun(left)
    
    f_right = fun(right)
    
    i = 1
    
    while (abs(f_left - f_right)>tol && i<1000){
      i = i + 1
      if (f_mid_1 < f_mid_2) {
        f_left = f_mid_1
        left = mid_1
      } else {
        f_right = f_mid_2
        right = mid_2
      }
      mid_1 = left + ratio * (right - left)
      f_mid_1 = fun(mid_1)
      mid_2 = mid_1 + ratio * (right - mid_1)
      f_mid_2 = fun(mid_2)
    }
    return(mean(mid_1,mid_2))
  }
```


```{r implement}
x_max = goose_egg(function(x) exp(x)*sin(x),range = c(0,1.5))

print(x_max)

plot(x_max,-exp(x_max)*sin(x_max))

plot(function(x) {-exp(x)*sin(x)}, xlim = c(0,1.5), add = T)

```


## Problem 2:

The Poisson distribution, written as  
$$P(Y=y) = \frac{\lambda^y e^{-\lambda}}{y!}$$
for $\lambda > 0$,
is often used to model ``count'' data ---
e.g., the number of events in a given time period. 

A Poisson regression model states that
$$Y_i \sim \textrm{Poisson}(\lambda_i),$$
where
$$\log \lambda_i = \alpha + \beta x_i $$
for some explanatory variable $x_i$.  The question is how to estimate $\alpha$ and $\beta$ given a
set of independent data $(x_1, Y_1), (x_2, Y_2), \ldots, (x_n, Y_n)$.


\begin{enumerate}
\item Generate a random sample $(x_i, Y_i)$ with $n=500$ from the Possion regression model above. 
You can choose the true parameters $(\alpha,\beta)$ and the distribution of $X$.

\item Write out the likelihood of your simulated data, and its Gradient and Hessian functions. 

\item  Develop a modify Newton-Raphson algorithm that allows the step-halving and re-direction steps
to ensure ascent directions and monotone-increasing properties. 

\item Write down your algorithm and implement it in R to estimate $\alpha$ and $\beta$ from your simulated data.
\end{enumerate}

# Answer: your answer starts here...

## 2.1
```{python eval =F}
print("hello world")
```

```{r}
X = rbind(rep(1,500),rnorm(500))
Beta = runif(2)
lambda = exp(t(X)%*%Beta)
Y = map(lambda,~rpois(1,.x)) %>% unlist()
dat = list(y = Y, x=X)
ans = glm(Y~0+t(X),family = poisson())
```

## 2.2

- The log-likelihood of Poisson distribution is 
$$l(Y;\lambda) = \sum \{y*log(\lambda) -\lambda - log(y!)\}$$ 
OR
$$l(Y;\alpha,\beta) = \sum \{y*(\alpha+x\beta) -exp(\alpha+x\beta) - log(y!)\}$$

- The Score funtion is 
$$\nabla(Y;\alpha,\beta) = \frac{\partial}{\partial\lambda}l(Y;\lambda) = (\sum\{y-exp(\alpha+x\beta)\},\sum\{y*x-x*exp(\alpha+x\beta)\})$$

- The hessian is 
$$\nabla^2(Y;\lambda) = \frac{\partial^2}{\partial\lambda^2}l(Y;\lambda)$$

$$
=
\begin{pmatrix}
\sum{-exp(\alpha+x\beta)} & \sum{-x*exp(\alpha+x\beta)} \\
\sum{-x*exp(\alpha+x\beta)} & \sum{-x^2*exp(\alpha+x\beta)}
\end{pmatrix}
$$
which is negative defined everywhere.

```{r poisson}
Poisson =
  function(Y,X,
           theta_vec) {
    lambda = exp(t(X) %*% theta_vec)
    loglink = sum(Y * log(lambda) - lambda - log(factorial(Y)))
    
    fisher = var(Y * log(lambda) - lambda - log(factorial(Y)))
    
    gradient = c(sum(Y - lambda), sum(Y * X[2,] - X[2,] * lambda))
    
    hessian = matrix(c(
      sum(-lambda),
      sum(-X[2,] * lambda),
      sum(-X[2,] * lambda),
      sum((-X[2,] ^ 2) * lambda)
    ), ncol = 2)
    
    return(list(
      loglink = loglink,
      fisher = fisher,
      gradient = gradient,
      hessian = hessian
    ))
  }

Poisson(Y,X,c(7,2))
```

## 2.3

the Newton method updating is:
$$\nabla g(x_{k+1}) = \nabla g(x_k) + \eta*\nabla^2 g(x_k)(x_{k+1}-x_k)$$
where $\eta$ is the step size that ensure $\nabla g(x_{k+1}) > \nabla g(x_k)$




```{r newton}
#Develop a modify Newton-Raphson algorithm that allows the 
#step-halving and 
#re-direction steps
#to ensure ascent directions and monotone-increasing properties. 

newton_update =
  function(fun,
           previous_theta,
           y,x,
           step_size = 1,
           optimizer = F,
           backtracking = T,
           tol = 1e-8) {
    #take previous gradient and a updated hessian, return update gradient with
    #backtracking
    #if (abs(fun(y,x,previous_theta)$loglink) == Inf) stop("Check your log-likelihood")
    trial = 0
    
    gradient = fun(y,x,previous_theta)$gradient
    
    if (is.function(optimizer)) {
      hessian = optimizer(y,x, fun,) # get H
    } else{
      if (is.numeric(optimizer)) {
        H = optimizer # use H
      } else{
        hessian = fun(y,x, previous_theta)$hessian
        H = solve(hessian)
        while (all(eigen(H)$values > 0)) {# eigen decomposition
          P = eigen(hessian)
          lambda = max(P$values)
          hessian =
            P$vectors %*% (diag(P$values) - (lambda + tol) * diag(length(P$values))) %*%
            solve(P$vectors)
          
          H = solve(hessian)
        }
      }
    }
    
    #updating
    cur_theta = previous_theta - step_size * H %*% gradient
    
    #backtracking
    while (backtracking & fun(y,x,cur_theta)$loglink < fun(y,x,previous_theta)$loglink & trial < 2000) {
      step_size = step_size / 2
      
      trial = trial + 1 # avoild dead loops
      
      cur_theta = previous_theta - step_size * H %*% gradient
    }
    
    return(cur_theta)
  }

newton_update(fun = Poisson,previous_theta = c(7,2), y=Y,x =X)
```

```{r echo = F, eval = F}
BFGS = function(theta_1,Y_1,theta_2,Y_2,theta_3,Y_3){
  H_old = 
}
```



```{r naive_newton}
naive_newton =
  function(fun,
           init_theta = 1,
           y,x,
           tol = 1e-8,
           maxtiter = 2000,
           optimizer = F,
           ...) {
    
    f = fun(y,x,init_theta)
    
    if (any(is.null(f$loglink),
            is.null(f$gradient),
            is.null(f$hessian))) {
      stop("fun input must return both gradient and hessian")
    }
    result = tibble()
    
    i = 0
    
    cur_theta = init_theta
    
    prevlog = -Inf # \nabla g(x_{k})
    
    while (any(abs(f$loglink)==Inf,abs(f$loglink - prevlog) > tol) && i < maxtiter) {
      i = i + 1
      prev_theta = cur_theta
      prevlog = f$loglink
      cur_theta = newton_update(fun, prev_theta,y,x)
      f = fun(y,x,cur_theta)
      result =
        rbind(result, tibble(
          iter = i,
          x_i = list(prev_theta),
          `g(x_i)` = prevlog
        ))
    }
    return(list(theta = cur_theta,result = result))
  }
```

```{r}
Beta_hat = naive_newton(Poisson,init_theta = c(7,2),Y,X)$theta

tibble(
  term = c("alpha", "beta"),
  theta = Beta,
  theta_hat = Beta_hat
) %>%
  knitr::kable()
```



## Problem 3: 
```{r echo = F}
breast = 
  read_csv("./breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  select(diagnosis:fractal_dimension_mean) %>% 
  mutate(diagnosis = forcats::fct_relevel(diagnosis,"M"))

Y = breast$diagnosis

X = model.matrix(diagnosis~.,breast)[,-1]
```


The data \textit{breast-cancer.csv} have 569 row and 33 columns. The first column \textbf{ID} lables individual breast tissue images; The second column \textbf{Diagnonsis} indentifies if the image is coming from cancer tissue or benign cases (M=malignant, B = benign). There are 357  benign and 212  malignant cases. The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei;
\begin{itemize}
\item radius (mean of distances from center to points on the perimeter)
\item texture (standard deviation of gray-scale values)
\item perimeter
\item area
\item smoothness (local variation in radius lengths)
\item compactness (perimeter\^ 2 / area - 1.0)
\item concavity (severity of concave portions of the contour)
\item concave points (number of concave portions of the contour)
\item symmetry
\item fractal dimension ("coastline approximation" - 1)
\end{itemize}


The goal  is to build a predictive model based on logistic regression to facilitate cancer diagnosis; 


\begin{enumerate}
\item Build a logistic model to classify the images into  malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.  

\item Build a logistic-LASSO model to select features, and implement a path-wise coordinate-wise optimization algorithm to obtain a path of solutions with a sequence of descending $\lambda$'s. 


\item Write a report to summarize your findings.
\end{enumerate}

# 3

## 3.1

the data is a binomial outcome response, which follows a Bernoulli distribution,
Using logit link, which 
$$log(\frac{p}{1-p}) = X^T\beta$$
s.t 
$$p = \frac{exp(X\beta)}{1+exp(X\beta)}$$
the log-likelihood of Bernoulli is 
$$l(Y;\beta) = \sum\{y*log(\frac{p}{1-p})+log(1-p)\}=\sum\{y*X^T\beta-log({1+exp(X^T\beta)})\}$$
The gradient is
$$\nabla l(Y;\beta) = (\frac{\partial}{\partial \beta_i}l(Y;\beta))=(\sum(y*x_i - \frac{x_i exp(X^T\beta)}{1+exp(X^T\beta)})$$
and the hessian is 
$$\nabla^2 l(Y;\beta)=(\sum  -\frac{x_i*x_j*exp(X^T\beta)}{(1+exp(X^T\beta)^2)})$$

```{r i_dont_know}
Bernoulli = 
  function(y,x,
           theta_vec){
    if (length(theta_vec)!=ncol(x)) stop("length of theta_vec must match dim of x")
    if (is.factor(y)) y = y %>% as.numeric()-1
    Y = y
    X = x
    loglink = sum(Y * X%*%theta_vec - log(1+exp(X%*%theta_vec)))
    
    if (abs(loglink) == Inf){
     stop("Choose a better starting value")
    }
    
    fisher = var(Y * X%*%theta_vec - log(1+exp(X%*%theta_vec)))
    
    X = x*1e-0
    
    gradient = map(1:length(theta_vec),
                   ~ sum(Y * X[, .x] - X[, .x] * exp(X %*% theta_vec) /
                           (1 + exp(X %*% theta_vec)))) %>% unlist()
    
    hessian =
      expand.grid(i = seq(1,length(theta_vec)),
                  j = seq(1,length(theta_vec))) %>% 
      summarise(beta = map2(i,j,~sum(-X[,.x]*X[,.y]*exp(X%*%theta_vec)/(1+exp(X%*%theta_vec))^2))) %>%
      unnest(beta) %>% 
      pull(beta)
    
    hessian = matrix(hessian,ncol = length(theta_vec))
                 
    return(
      list(loglink = loglink,
           fisher = fisher,
           gradient  = gradient*1e+0,
           hessian = hessian*1e+0)
    )
  }

 Bernoulli(Y,X,rep(0,10))
# Bernoulli(dat,ans$coefficients[-1] %>% as.vector())
```
## 3.2


```{r lasso}
lasso_update_fit =
  function(fun, y,x, theta_vec = NaN,lambda = 1,maxiter = 200,tol = 1e-6,... ) {
    # data preprocessing
    if (is.factor(y)) y = y %>% as.numeric() -1
    y = y
    x = scale(x)
    xsd = attr(x,"scaled:scale") %>% as.vector()
    x = cbind(rep(1,length(y)),x) # add alpha
    xsd = append(1,xsd)
    # checking if intercept is include
    beta_0 = mean(y)/(1-mean(y)) %>% log()
    if (any(is.na(theta_vec))) theta_vec = rep(beta_0,ncol(x))
    if (length(theta_vec)<ncol(x)) theta_vec = append(beta_0,theta_vec)

    iter = 0
    
    soft_threshold =
      function(beta, lambda){
        beta = (abs(beta) > lambda) * (beta - sign(beta) * lambda) +(abs(beta) < lambda) * 0
        return(beta)}
    
    cur_result = fun(y,x, theta_vec)$loglink
    
    prev_result = -Inf
    
    result = tibble()
    while (any(abs(cur_result) == Inf,abs(cur_result - prev_result) > tol) && iter < maxiter) {
      prev_result = cur_result
      iter = iter + 1
      
      for (i in 1:length(theta_vec)) {
        #coordinate update beta not intercept
        theta_old = theta_vec[i] 
        cur_f = fun(y,x, theta_vec)
        cur_result = cur_f$loglink
        gradient_i = cur_f$gradient[i]
        hessian_i = cur_f$hessian[i, i]
        H_i = solve(hessian_i)
        if (H_i>0) H_i = -1
        theta_new = theta_old - H_i %*% gradient_i
        if (i>1) theta_new = soft_threshold(theta_new, lambda) 
        # don't penalize intercept
        theta_vec[[i]] = theta_new
      }

     # result = rbind(result,
     #                tibble(
     #                  iteration = iter,
     #                  theta_id = list(1:length(theta_vec)),
     #                  theta = list(theta_vec*xsd),
     #                  L1 = fun(y,x, theta_vec)$loglink
     #                ))
    }
    return(list(theta = theta_vec/xsd, result = result))
  }

lasso_update = function(fun, y,x, theta_vec = NaN,
                        lambda = exp(seq(from = 5, to = -10)), ...) {
  lambda = lambda %>% as.vector()
  result = tibble()
  for (lambda_i in lambda) {
    result = rbind(result,
                   tibble(
                     lamdba = lambda_i,
                     result = lasso_update_fit(fun, y, x, theta_vec = theta_vec, lambda = lambda_i)
                   ))
  }
  return(result)
}


b = glmnet(X,Y,family="binomial",lambda = 0.1)

tibble(
  newton = naive_newton(Bernoulli, init_theta = rep(0, 11), Y, cbind(rep(1, length(
    Y
  )), X))$theta,
  lasso = lasso_update_fit(Bernoulli,Y,X,lambda = 0.5)$theta,
  glmnet = coef(b) %>% as.vector(),
  glm = glm(diagnosis ~ ., data = breast, family = binomial())$coefficient %>% as.vector()
) %>%
  knitr::kable()
```


```{r quadratic_lasso}
Qlasso =
  function(y,
           x,
           lambda = 0,
           maxiter = 200,
           tol = 1e-6,
           ...) {
    # data preparation
    if (is.factor(y))
      y = y %>% as.numeric() - 1
    y = y
    x = scale(x)
    xsd = attr(x, "scaled:scale") %>% as.vector()
    x = cbind(rep(1, length(y)), x) # add alpha
    xsd = append(1, xsd)
    
    # Manually choosing starting value
    theta_vec = rep(0, ncol(x))
    
    
    #update part
    iter = 0
    
    # quadratic function
    
    qfun =
      function(y, x, theta_vec) {
        loglink = -sum(w_prev * (z_prev - x %*% theta_vec) ^ 2) / (2 * length(y))
        
        p_prev = exp(x %*% theta_vec) / (1 + exp(x %*% theta_vec)) # 1 * col matrix
        
        w_prev = p_prev * (1 - p_prev) # 1 * col matrix
        
        z_prev = x %*% theta_vec + (y - p_prev) / w_prev # row * 1 matrix
        
        #gradient
        gradient_prev =
          map(1:ncol(x),
              ~ sum(w_prev * (z_prev - x %*% theta_vec) * x[, .x]) / length(y)) %>%
          unlist()# col * 1 data
        
        # Hessian
        hessian_prev =
          expand.grid(i = 1:ncol(x),
                      j = 1:ncol(x)) %>%
          map2(i, j,
               ~ -sum(w_prev * (x[, .x] * x[, .y])) / length(y)) %>%
          unlist()
        
        return(
          list(
            loglink = loglink,
            p = p_prev,
            w = w_prev,
            z = z_prev,
            gradient = gradient_prev,
            hessian = hessian_prev
          )
        )
      }
    
    cur_result = qfun(y, x, theta_vec)
    
    if (abs(cur_result) == Inf)
      stop("Diverge at starting value")
    
    prev_result = -Inf
    
    while (abs(cur_result - prev_result) > tol
           && iter < maxiter) {
      iter = iter + 1
      
      prev_result = cur_result
      
      for (i in 1:length(theta_vec)) {
        #weight
        fun_prev = qfun(y, x, theta_vec)
        p_prev = fun_prev$p
        z_prev = fun_prev$z
        w_prev = fun_prev$w
        gradient_prev = fun_prev$gradient
        hessian_prev = fun_prev$hessian
        H_prev = solve(hessian_prev[i, i])
        
        #update
        cur_theta =
          theta_vec[[i]] - H_prev * gradient_prev[[i]]
        
        #soft-threshod,skip penalize intercept
        if (i > 1)
          cur_theta =
          (abs(cur_theta) > lambda) * (cur_theta - sign(cur_theta) * lambda) +
          (abs(cur_theta) < lambda) * 0
        
        #update theta
        theta_vec[[i]] = cur_theta
      }
      
      cur_result = qfun(y, x, theta_vec)
      
    }
    
    return(theta_vec*xsd)
  }

tibble(a = Qlasso(y,x),
       b = Qlasso(y,x,1))

```

