---
title: "Homework on re-sampling methods"
date: "P8160 Advanced Statistical Computing "
output: pdf_document #html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(survival)
library(foreach)
library(parallel)
library(doParallel)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)
library(tidyverse)

set.seed(2021)
```
**In this homework, please using parallel computing codes for your implementations.**

## Problem 1: a randomized trial on an eye treatment

An ophthalmologist deisgned a randomized clinical trial to evaluate a new laser treatment in comparison to the traditional one.  The response is visual acuity, measured by the number of letters correctly identified in a standard eye test.  20 patients have both eyes eligible for laser treatment. The ophthalmologist randomized the two laser treatments (new vs traditional) to the two eyes of those patients (i.e. one eye received the new laser treatment and the other receive traditional laser treatment). Another 20 patients had only one suitable eye, so they received one treatment allocated at random.So we have a mixture of paired comparison and two-sample data.

```{r}
blue <- c(4,69,87,35,39,79,31,79,65,95,68,
           62,70,80,84,79,66,75,59,77,36,86,
           39,85,74,72,69,85,85,72)
red <-c(62,80,82,83,0,81,28,69,48,90,63,
        77,0,55,83,85,54,72,58,68,88,83,78,
        30,58,45,78,64,87,65)
acui<-data.frame(str=c(rep(0,20),
            rep(1,10)),red,blue)
```


\vskip 20pt

\textbf{Answer the following question:}
 
\begin{enumerate}
\item[(1)]  The treatment effect of the new laser treatment is defined as $$E(Y\mid \mbox{trt = new}) - E(Y\mid \mbox{trt = traditional}).$$  Estimate the treatment effect using the collected data. 
\item[(2)] Use bootstrap to construct 95% confidence interval of the treatment effect. Describe your bootstrap procedure, and what is your conclusion from the bootstrap CI?
\end{enumerate}

### 1
 $$E(Y\mid \mbox{trt = red}) - E(Y\mid \mbox{trt = blue}).$$ 

```{r}
mean(red-blue)
```
### 2

```{r}
bootr = 
  function(data,return_sample = F){
    bt = sample(data,length(data),replace = T)
    bt_mean = mean(bt)
    bt_bias = mean(data) - mean(bt)
    bt_sd = sd(bt)
    
    if(!return_sample) bt = NA
    
    return(list(
      sample = bt,
      mean = bt_mean,
      sd = bt_sd,
      bias = bt_bias
    ))
  }
```

```{r bootstrap}
#1e+4 bootstrap

cl = makePSOCKcluster(5)
registerDoParallel(cl)

B = 1e+4

red_bt = foreach(i = 1:B,
                 .combine = rbind,
                 .inorder = F) %do% bootr(red)$mean

blue_bt = foreach(i = 1:B,
                 .combine = rbind,
                 .inorder = F) %do% bootr(blue)$mean

diff_bt = red_bt - blue_bt

hist(diff_bt)

diff_sd_bt = sqrt(var(diff_bt))

stopCluster(cl)
```



```{r}
alpha = 0.05
lower = max(diff_bt[percent_rank(diff_bt)<alpha/2])
upper = min(diff_bt[percent_rank(diff_bt)>(1-alpha/2)])
lower_aj = 2*mean(red-blue) -upper
upper_aj = 2*mean(red-blue) - lower
```
The unadjusted CI is `r `c(lower,upper)`.The adjusted 95% CI is`r c(lower_aj,upper_aj)`.Given the bootstrap interval, we see that 0 is within the CI, we cannot reject the Null hypothesis that the interventions have no difference.


## Problem 2 (Continue from Homework 3, the breast cancer sutdy): 


The data \textit{breast-cancer.csv} have 569 row and 33 columns. The first column \textbf{ID} lables individual breast tissue images; The second column \textbf{Diagnonsis} indentifies if the image is coming from cancer tissue or benign cases (M=malignant, B = benign). There are 357  benign and 212  malignant cases. The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei;
\begin{itemize}
\item radius (mean of distances from center to points on the perimeter)
\item texture (standard deviation of gray-scale values)
\item perimeter
\item area
\item smoothness (local variation in radius lengths)
\item compactness ($perimeter^2/area -1$)
\item concavity (severity of concave portions of the contour)
\item concave points (number of concave portions of the contour)
\item symmetry
\item fractal dimension ("coastline approximation" - 1)
\end{itemize}


In homework 3, you have developed a path-wise coordinate-wise optimization algorithm for logistic LASSO regressions. Using the algorithm you developed there, complete the following tasks 


\begin{enumerate}
\item Propose and implement a 5-fold cross-validation aglorithm to select the turning parameter in the logistic LASSO regression.  We call the logistic-LASSO with CV-selected $\lambda$ as the "optimal" logistic LASSO


\item Using the selected predictors from the  "optimal" logistic LASSO to predict the probability of malignant for each of the images (Note that estimates from logistic-Lasso are biased. You need to re-fit the logistic regression with the selected predictors to estimate the probability.)  How well the predictors classify the images?

\item Using the bootstraping smoothing idea to re-evaluate the probabilities of malignant. How well the new predictors classify the images?
\end{enumerate}

```{r load_lasso_function,warning=F,message=F}
breast = 
  read_csv("./breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  select(-id,-x33) %>% 
  mutate(diagnosis = forcats::fct_relevel(diagnosis,"M")
         )

Y = breast$diagnosis

X = model.matrix(diagnosis~.,breast)[,-1]

set.seed(123123)

test_index = sample(1:length(Y),0.2*length(Y))

corrplot::corrplot(cor(X))

high_cor = caret::findCorrelation(cor(X),0.8)

X = X[,-high_cor]

Y_ts = Y[test_index]

X_ts = X[test_index,]

Y_tr=Y[-test_index]

X_tr=X[-test_index,]
```

Keeping only variables below correlation of 0.8.

Using predictive error rate as loss function, we tune the lasso logistic as:

```{r model_fitting}
set.seed(123123)
source("lasso_function.R")

a = cv_Qlasso(Y_tr,X_tr,bfun,intercept = T,return_scale = T)

a

drop_var = a$coefficient==0

names(breast)[-high_cor][-drop_var]

X_new = as_tibble(X_tr[,-drop_var])

predict_model = glm(Y_tr~.,data=X_new,family = binomial())
```


```{r prediction}
X_ts_new = X_ts[,-drop_var]

pd = ifelse(predict(predict_model,newdata = as_tibble(X_ts_new),type = "response")>=0.5,"B","M") %>% forcats::fct_relevel(.,"M") %>% as.numeric() -1

Y_ts

mean(Y_ts %>% as.numeric() -1 !=pd)

plot(roc(Y_ts %>% as.numeric() -1,pd),legacy.axes = T)
```



```{r}
test_data = 
  cbind(Y_tr,X_new) %>% 
  as_tibble() %>% 
  rename(y=Y_tr)

cl = makePSOCKcluster(5)
registerDoParallel(cl)

bt_err = foreach(i=1:B,
                 .combine = rbind) %dopar% {
                   bt_data= dplyr::sample_n(test_data,nrow(test_data),replace = T)
                   bt_md = glm(y~.,data = bt_data,family = binomial)
                   bt_pd = rep("M",len = length(Y_ts))
                   bt_pd[predict(bt_md,dplyr::as_tibble(X_ts_new),type = "response")>0.5]="B"
                   mean(Y_ts!=bt_pd)
                 }

stopCluster(cl)

hist(bt_err)

abline(v = mean(bt_err),add = T,col=2)
```



```{r smoothing,include=F,eval=F}
source("golden_search.R")

Y_ts = as.numeric(Y_ts)-1

pd = as.numeric(pd) -1

btsm =
  function(data,width) {
    den = density(data, bw = width)
    
    den.s = smooth.spline(den$x, den$y, all.knots = T, spar = 0.8)
    
    s.1 = predict(den.s, den.s$x, deriv = 1)
    
    nmode = length(rle(den.sign <- sign(s.1$y))$values) / 2
    
    return(list(nmode = nmode))
  }


find_h = 
  function(data,left=1e-4,right=1e+4){
    goose_egg(fun = 
                function(x) -abs(btsm(data,x)$nmode),
              left = left,
              right = right)
  }

boot_H = 
  foreach(i = 1:B,
          .combine = rbind) %do%{
            find_h(sample(Y_ts,length(Y_ts),replace = T))
          }
```




