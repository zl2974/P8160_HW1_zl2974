---
title: "Homework 5 on MCMC"
author: "ZHUOHUI LIANG zl2074"
date: "Due: 04/18/2020, by 11:59pm"
output: pdf_document
---

```{r setup, include=FALSE}
library(parallel)
library(foreach)
library(doParallel)
library(tidyverse)



knitr::opts_chunk$set(echo = TRUE,
                      eval = T)
```

# Problem 1
Derive the posterior distributions in the following settings:

1. Suppose $X_1,...,X_n$ iid sample from $N(\theta, \sigma^2)$ distribution, the prior distribution of $\theta$ is $N(\mu, \tau^2)$, derive the posterior distribtuion of $\theta$ given $\mathbf{X}$:

\begin{eqnarray}
Pr[\theta|X] = \frac{Pr[\theta]*Pr[X|\theta]}{Pr[X]}
 &\propto& exp(-\frac{(\mu-\theta)^2}{\tau^2})*exp(-\frac{(\sum X-\theta)^2}{n\sigma^2})\\
 &=& exp[\frac{-(n\sigma^2(\mu^2-2\mu\theta+\theta^2)+\tau^2(\sum X^2-2\sum X\theta+\theta^2))}{\tau^2n\sigma^2}]\\
 &\propto& exp(-[\frac{\theta^2-2\theta(\frac{\mu}{\tau^2}+\frac{\sum X}{n\sigma^2})+(\frac{\mu}{\tau^2}+\frac{\sum X}{n\sigma^2})^2}{(\frac{1}{\tau^2}+\frac{n}{\sigma})^{-1}}])
\end{eqnarray}

As such, $\theta|X\sim N((\frac{\mu}{\tau^2}+\frac{\sum X}{n\sigma^2}),(\frac{1}{\tau^2}+\frac{n}{\sigma})^{-1})$


2.Suppose $X_1,...,X_n$ iid sample from $U(0, \theta)$ distribution, the prior distribution of $\theta$ is Pareto distribution with pdf
$$\pi(\theta) = \frac{\alpha \beta^\alpha}{\theta^{\alpha+1}}I\{\theta\ge \beta\}$$ with known $\beta$ and $\alpha$


\begin{eqnarray}
Pr[\theta|X] &\propto& L(X|\theta)*\pi(\theta)\\
 &=& \theta^{-n}*\frac{\alpha\beta^\alpha}{\theta^{\alpha+1}}I(\theta\ge\beta)\\
 &=& \frac{\alpha\beta^\alpha}{\theta^{n+\alpha+1}}I(\theta\ge\beta)
\end{eqnarray}


# Answer: your answer starts here...

```{r }
#R codes:
```

# Problem 2
Suppose there are three possible weathers in a day: rain, nice,
cloudy. The transition probabilities are

rain nice cloudy

rain 0.5 0.5 0.25

nice 0.25 0 0.25

cloudy 0.25 0.5 0.5

where the columns represent the ``origin" and the rows represent the
``destination" of each step. The initial probabilities of the three states are
given by (0.5,0, 0.5) for (rain, nice, cloudy). Answer the following questions


1.  Compute the probabilities of the three states on the next step of the chain.

2.  Find the stationary distribution of the chain

3.  Write an R algorithm for the realization of the chain and illustrate the
feature of the chain.

# Answer: your answer starts here...

```{r }
K = matrix(c(.5, .5, .25, .25, 0, .25, .25, .5, .5), 3, 3, byrow = T)

init = c(0.5, 0, .5)


# function
easychain = function(init, transit, step = Inf) {
  
  state = K %*% init
  
  prev_state = init
  
  i = 1
  while (any(abs(prev_state - state) > 1e-6 & i < step)) {
    i  = i + 1
    prev_state = state
    state = K %*% state
  }
  return(state)
}
```

```{r }
# first step
easychain(init,K,1)

```

```{r }
# converge value/ stationary
easychain(init,K) 

# proved of stationary
easychain(c(.1,.3,.6),K)

easychain(diag(3),K)

```


\paragraph{problem 3}

Consider the bivariate density
$$f(x,y) \propto {n \choose x} y^{x + a - 1}(1- y)^{n-x+b-1} , x=0, 1, \cdots, n, 0\le y \le 1$$
Complete the following tasks:

1. Write the algorithm of the Gibbs sampler,  implement it in R program, and generate a chain with target joint density $f(x,y)$

2. Use a Metropolis sampler to generate a chain with target joint density $f(x; y)$ and implement in R program.

3. Suppose $n = 30, a = 9, b = 14$, use simulations to compare the performance of the above two methods.

# Answer: your answer starts here...

## 1

\begin{eqnarray}
f(x|y) &=& \frac{f(x,y)}{f(y)}\\
 &=& f(x,y)/\sum_xf(x,y)\\
 &=& f(x,y)/y^{a-1}(1-y)^{b-1}\sum {n \choose x} y^x(1-y)^{n-x}\\
 &=& \frac{{n \choose x} y^{x + a - 1}(1- y)^{n-x+b-1}}{y^{a-1}(1-y)^{b-1}}\\
 &=& Bin(n,y)
\end{eqnarray}


\begin{eqnarray}
f(y|x) &=& \frac{f(x,y)}{f(x)}\\
 &=& f(x,y)/{n \choose x}\int_yy^{x+a-1}(1-y)^{n-x+b-1}\\
 &=& f(x,y)/{n \choose x}B(x+a,n-x+b)\\
 &=& \frac{{n \choose x} y^{x + a - 1}(1- y)^{n-x+b-1}}{{n \choose x}B(x+a,n-x+b)}\\
 &=& Beta(x+a,n-x+b)
\end{eqnarray}

```{r }
gibbs =
  function(n,
           a,
           b,
           step = 1e+4,
           burn = F,
           x_init = NA,
           y_init = NA,
           .tol = 1e-6) {
    if (is.na(x_init))
      x_init = runif(1,1,10)%/%1
    
    if (is.na(y_init))
      y_init = runif(1)
    
    x = c(x_init)
    y = c(y_init)
    iter = 1
    
    while (iter < step) {
      x = c(x, rbinom(1, n, y[iter]))
      y = c(y, rbeta(1, x[iter] + a, n - x[iter] + b))
      iter = iter + 1
    }
    
    
    index = 1:step
    
    if (burn) {
      index = index[-c(1:burn)]
      x = x[-c(1:burn)]
      y = y[-c(1:burn)]
    }
    
    return(list(x = x,
                y = y,
                index = index))
  }


set.seed(123123)
test_gibbs = gibbs(30,a = 9,b=14)

ggplot(as_tibble(test_gibbs))+geom_path(aes(index,x))

hist(test_gibbs$x)

ggplot(as_tibble(test_gibbs))+geom_path(aes(index,y))

hist(test_gibbs$y)
```


## 2

We propose two different proposal distribution for x and y,

$$Y|X \sim Beta(X+a,n-X+b)$$

and 

$$X|Y \sim Poisson(n*Y)$$

s.t over accept probabilty is:

$$\alpha_i(x_i^k,X_{-i}^{k},y_i) = min\{\frac{\pi(poison(y_1;n*y_2))*\pi(beta(y_2;y_1+a,n-y_1+b)){n \choose y_1} y_2^{y_1 + a - 1}(1- y_2)^{n-y_1+b-1} }{\pi(poison(x_1;n*x_2))*\pi(beta(x_2;x_1+a,n-x_1+b)){n \choose x_1} x_2^{x_1 + a - 1}(1- x_2)^{n-x_1+b-1} };1\})$$


```{r}
logP = function(theta,n,a,b){
  x = theta[[1]]
  y = theta[[2]]
  
  res = choose(n, x) *
          y ^ (x + a - 1) *
          (1 - y) ^ (n - x + b - 1)
  res = dpois(x,n*y)*res
  
  res = dbeta(y,x+a,n-x+b) * res
  
  return(log(res))}

x_update = 
  function(x,y,n,a,b,...){
    new_x = 
      rpois(1,n*y)
  }

y_update = 
  function(x,y,n,a,b,...){
    # make sure that y is in 0,1
    new_y = 
      rbeta(1,x+a,n-x+b)
    return(new_y)
  }

M_update =
  function(theta,update_function_list, n, a, b) {
    for (i in 1:length(theta)) {
      # take old parameter
      new = theta
      
      #update x/y given old
      new[[i]] = update_function_list[[i]](theta[[1]],theta[[2]],n,a,b)
      
      #calculated the acceptance rate
      accept = logP(new, n, a, b) - logP(theta, n, a, b)
      
      if(is.na(accept)) next
      
      if (log(runif(1)) < accept)
        theta = new
    }
    
    return(theta)
  }

MET =
  function(n,
           a,
           b,
           step = 1e+4,
           .tol = 1e-6,
           x_init = 1,
           y_init = .5,
           burn = F,
           ...) {
    iter = 1
    
    x = y = xaccept = yaccept = rep(NA, step)
    
    x[[1]] = x_init
    
    y[[1]] = y_init
    
    
    while (iter < step) {
      new_theta = M_update(c(x[[iter]], y[[iter]]),
                           list(x_update, y_update),
                           n, a, b)
      iter = iter + 1
      x[[iter]] = new_theta[[1]]
      xaccept[[iter]] = x[[iter - 1]] != x[[iter]]
      y[[iter]] = new_theta[[2]]
      yaccept[[iter]] = y[[iter - 1]] != y[[iter]]
    }
    
    if (burn) {
            x = x[-c(1:burn)]
            y = y[-c(1:burn)]
            xaccept = xaccept[-c(1:burn)]
            yaccept = yaccept[-c(1:burn)]
    }
    
    accept =
            list(x = xaccept,
                 y = yaccept)

return(list(x = x,
            y = y,
            accept = accept))
  }

re = MET(30,4,13,step = 1000)

ggplot(tibble(index = 1:1000, x = re$x),aes(x =index, y = x))+
  geom_path()+
  labs(title = str_c("acceptance is ",sum(re$accept$x,na.rm = T)/1000))

ggplot(tibble(index = 1:1000, y = re$y),aes(x =index, y = y))+
  geom_path()+
  labs(title = str_c("acceptance is ",sum(re$accept$y,na.rm = T)/1000))
```

```{r}
# n=30,a = 9, b = 14

#starting value x: 1 to 30
#y 0 to 1

set.seed(123123)
cl = makePSOCKcluster(5)
registerDoParallel(cl)

cond = expand.grid(x_int = seq(1, 30, len = 10) %/% 1,
                     y_int = seq(0, 1, len = 5))
  
G = foreach(i = 1:nrow(cond),
              .combine = rbind) %dopar% {
                x = cond[i, 1]
                y = cond[i, 2]
                g_mean = list()
                iter = 1
                while(iter<100){
                  g = gibbs(
                  30,
                  9,
                  14,
                  step = 1000,
                  x_init = x,
                  y_init = y,
                  burn = 100
                )
                g_mean[[iter]] = as.numeric(lapply(g, mean)[-3])
                iter = iter + 1
                }
                g_mean = do.call(rbind,g_mean)
                
                g_mean = colMeans(g_mean)
                
                g_mean
              }
  
M = foreach(i = 1:nrow(cond),
              .combine = rbind) %dopar% {
                x = cond[i, 1]
                y = cond[i, 2]
               m_mean = list()
               iter = 1
               while (iter < 100) {
                  m = MET(
                  30,
                  9,
                  14,
                  step = 1000,
                  x_init = x,
                  y_init = y,
                  burn = 100
                )
                m_mean[[iter]] = as.numeric(lapply(m[-3], mean))
                iter = iter + 1
               }
                m_mean = colMeans(do.call(rbind,m_mean))
                
                m_mean
              }

stopCluster(cl)

sim_data = cbind(cond,G,M)

names(sim_data) = c("x_init","y_init","gibbs_x","gibbs_y","met_x","met_y")

knitr::kable(sim_data,
             caption = "Simulation result based on 100 run on differet start values with 100 burn")
```

